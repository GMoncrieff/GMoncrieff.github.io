[
  {
    "path": "posts/2021-09-22-sheduled-earth-engine/",
    "title": "Automate your Google Earth Engine analyses",
    "description": "Using Google Cloud Platform to run Earth Engine scripts on an predefined schedule",
    "author": [
      {
        "name": "Glenn Moncrieff",
        "url": {}
      }
    ],
    "date": "2021-11-27",
    "categories": [],
    "contents": "\nGoogle Earth Engine (GEE) is a great tool for analyzing earth observation data and producing new insights into environmental change at very large scales. An issue that I have bumped into more than once after producing a neat new analysis is that the results soon become stale as new earth observation data become available. For example, if I produce a fresh new model to map recent fires in my country using Sentinel 2 data, within a few days new images will be come available and my fire map will be out of date. Another instance in which it might be useful to schedule our analyses to run automatically and save output is when we have a user facing Earth Engine app that runs some analyses on data in the GEE catalog. If our calculations are complicated it can take a long time to render the results for people using the app. But if the data is precalculated, the visualization will load in no time.\nUntil now in order to precalculate updates results and save them I had to manually rerun my scripts and export the results. In this short post I will explain how to setup a scheduler running in Google Cloud Platform (GCP) that will fire off our analysis at the frequency of our choosing and save the results to an Earth Engine asset. Of course we could do this by setting up a scheduler on our own computer using the cron command-line utility, but it costs nothing - $0 - to run on this GCP.\nSetup\nBefore we start lets create a new folder in which we will save all our code\n\nmkdir ee_function\ncd ee_function\n\nAs a prerequisite you will need a GCP account. If you don’t have one you can register here and you will start out with $300 of free credits.\nMost of what we are going to do here using the command-line can also be done via the pointy-clicky method on the GCP web console, but for the sake of simplicity and reproducibility we will use GCP gcloud commands for everything. Instructions for installing gcloud can be found here. Once installed you will need to setup our installation using\n\ngcloud init\n\nYou will need to provide your login details, a default project, and a default zone.\nOnce setup we need to activate the various GCP APIs that we will be using. This is set project-wide, so if we are using an existing project some of these may already be activated. The GCP APIs we will be using are Pub/Sub, Cloud Functions, Cloud Scheduler, and Earth Engine. The following command will activate all these APIs\n\ngcloud services enable pubsub.googleapis.com\ngcloud services enable cloudfunctions.googleapis.com\ngcloud services enable earthengine.googleapis.com \ngcloud services enable cloudscheduler.googleapis.com\n\nNaturally, to use Earth Engine you need to sign-up. In order to allow an application (rather than a human end-user) to access Earth Engine through our account, we need to setup a service account for our GCP project and register this on our Earth Engine account (more info here). The first step is to create the service account\n\ngcloud iam service-accounts create ee-function \\\n--description=\"testing_ee_function\" \\\n--display-name=\"test_ee_function\"\n\nNext we create a key for the service account and download it locally as a json file.\n\ngcloud iam service-accounts keys create sa-private-key.json \\\n--iam-account=ee-function@ee-vegetation-gee4geo.iam.gserviceaccount.com\n\nThe last registration step is to register the service account we used above for Earth Engine access here. In this example our service account address is ee-function@ee-vegetation-gee4geo.iam.gserviceaccount.com i.e. SERVICEACCOUNT@PROJECTNAME.iam.gserviceaccount.com\nEarth Engine Cloud Function\nNow we get to the meat and potatoes - preparing our Earth Engine script. Our script will be deployed as a single function by Google Cloud Functions. We will therefore wrap our Earth Engine code inside a python function. The framework of our .py file will look like\n\nimport package\n\ndef main(*args, **kwargs):\n  value = \"value\"\n  return value\n\nWe simply need to import the packages our function requires and place our Earth Engine script inside the function main. The actual content of our Earth Engine script is not important in this example. For demonstration purposes here all we will do is calculate the median reflectance of all Sentinel 2 images in the previous 30 days within a set region. An important step here is determining today’s date using the python datetime package and converting this to an Earth Engine date. This allows us to modify the date over which our analysis is done based on the date on which it is run by the scheduler. At the end of the script we set the time_start property of the image to the date on which the script was called and export it, adding to an existing ImageCollection.\n\nimport ee\nimport datetime\n\ndef main(*args, **kwargs):\n\n  #initialize earth engine using json key for service account\n    service_account = 'ee-function@ee-vegetation-gee4geo.iam.gserviceaccount.com'\n    credentials = ee.ServiceAccountCredentials(service_account, 'sa-private-key.json')\n    ee.Initialize(credentials)\n\n  #our AOI\n    geom = ee.Geometry.Polygon([[[18.430231010200725, -34.051410739766304],[18.430231010200725, \\\n    -34.07871402933222],[18.4563235394976, -34.07871402933222],[18.4563235394976, -34.051410739766304]]])\n\n  #get current date and convert to ee.Date\n    end_date_str = datetime.datetime.today().strftime('%Y-%m-%d')\n    end_date = ee.Date(end_date_str)\n    start_date = end_date.advance(-1,'month')\n  \n  #calculate median Sentinel 2 reflectance\n    im = ee.ImageCollection(\"COPERNICUS/S2_SR\")\\\n    .filterBounds(geom)\\\n    .filterDate(start_date,end_date)\\\n    .median()\n  \n  #record the date by setting it as a property\n    im = im.set({'system:time_start':end_date})\n\n  #export by adding to an existing EE imageCollection\n    folder ='projects/ee-vegetation-gee4geo/assets/test2/'\n    filename = folder + 'exampleExport_'+ end_date_str\n\n    task = ee.batch.Export.image.toAsset(image=im,   \n                                         region=geom, \n                                         assetId=filename, \n                                         scale=100)\n\n    task.start()\n\n    return task\n\nWe need to create a requirements.txt file listing the packages our function requires. For this example this is just the earthengine-api, as datetime is installed by default in Python 3.7, and the file contents will simply be\nearthengine-api\nNow we are ready to upload the function to GCP and deploy using Cloud Functions. Double check that your current working directory contains the following files only:\nour python function: main.py\nthe requirements: requirements.txt\nthe json key: sa-private-key.json\n\ngcloud functions deploy ee-function \\\n--runtime=python37 \\\n--entry-point=main \\\n--trigger-topic=ee_topic\n\nIf all goes well our function should deploy.\nSetting a trigger and schedule\nWe can view the function and it’s details on the Cloud Functions tab of our GCP web console. When we deployed the function we set a trigger that will cause the function to execute --tigger-topic=ee_topic. This created a GCP Pub/Sub topic called ee_topic. Pub/Sub is GCP’s messaging service which integrates different components, some of which publish messages, and others that consume/subscribe to these messages. Our function consumes messages published to ee_topic, which will trigger it’s execution.\nAll we need to do now is publish messages to ee_topic and this will trigger our function. There are a number of ways to do this in GCP. For example we could trigger our function through an HTTP request, or when a new object is added to a Cloud Storage bucket (this is a nice way of linking multiple scripts that depend on the output of each other). Here we will use a scheduler which publishes messages to ee_topic and triggers our function on a regular predefined schedule.\nTo do this we will use Cloud Scheduler. The argument --schedule determines how often to publish to the ee_job topic, and we use the well known cron syntax. Crontab is a nice website which helps convert your schedule into cron syntax if you are not familiar with it. Here we simply publish once a month\n\ngcloud scheduler jobs create pubsub ee_job \\\n--schedule=\"0 0 1 * *\" \\\n--topic=ee_topic \\\n--message-body=\"run_ee\"\n\nThanks for stopping by\nAll done! Our function will now execute once a month every month for eternity. We can also manually trigger the execution of our function by visiting the Cloud Scheduler tab on our GCP cloud console. The best thing about the entire workflow? It is completely free. Earth Engine is free, Cloud Functions are free (up to 2 million invocations / month), Pub/Sub is free (up to 10 Gb / month) and Cloud Scheduler is free (up to 3 independent schedules per month). We are very unlikely to exceed these limits when using these service to automate Earth Engine analyses.\nIn summary we have put together a workflow for deploying any Earth Engine script to run on a schedule. There are many use cases for this, particularly when the end-use of our analysis is a product or insight that is needed for up-to-date decision making, or when we want to ensure that any downstream task that uses our output is making use of the most recent data.\n\n\n\n",
    "preview": "posts/2021-09-22-sheduled-earth-engine/clock.jpeg",
    "last_modified": "2022-10-07T11:52:35+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-08-12-advi-vs-mcmc/",
    "title": "ADVI vs MCMC",
    "description": "Comparing the performance of models fitted to vegetation postfire recovery data using ADVI and MCMC",
    "author": [
      {
        "name": "Glenn Moncrieff",
        "url": {}
      }
    ],
    "date": "2020-08-11",
    "categories": [
      "Bayesian",
      "Fire recovery"
    ],
    "contents": "\nMarkov Chain Monte Carlo (MCMC) is the workhorse of modern statistics. Efficient implementations in languages such as Stan, JAGS, and PyMC3 and many others mean that nowadays we can simply focus on the statistical model and ecological problems we are interested in and leave the estimation of posterior probabilities to be worked out by the MCMC sampler. There is, however, one MAJOR drawback to MCMC\nIT. IS. SLOW.\nI mean, when you understand what it is really doing under the hood it is actually doing this really, really fast. It is plenty fast for the majority of analyses - those that do not involve estimating 1000s of parameters. But it does not feel that way when you are trying to fit a model with a parameter for every sites on a large spatial grid or every point in a long time-series.\nA project that I have been working on for a few years involves estimating the postfire recovery of vegetation in the Cape Floristic Region (CFR) of South Africa. Objectivity the most beautiful vegetation on earth\nCape Floristic RegionThe details are given in (Slingsby, Moncrieff, and Wilson 2020; Wilson, Latimer, and Silander 2015), but in short what we do is estimate the age of a site by calculating the years since the last fire. We then fit a curve to model the recovery of vegetation (measured using NDVI) as a function of it’s age. For this we use a negative exponential curve with the following form:\n\\[\\mu_{i,t}=\\alpha_i+\\gamma_i\\Big(1-e^{-\\frac{age_{i,t}}{\\lambda_i}}\\Big)\\]\nwhere \\(\\mu_{i,t}\\) is the expected NDVI for site \\(i\\) at time \\(t\\)\nThe observed greenness \\(NDVI_{i,t}\\) is assumed to follow a normal distribution with mean \\(\\mu_{i,t}\\) \\[NDVI_{i,t}\\sim\\mathcal{N}(\\mu_{i,t},\\sigma_)\\]\nAn additional level models the parameters of the negative exponential curve as a function of environmental variables. This means that sites with similar environmental conditions should have similar recovery curves. The full model also includes a sinusoidal term to capture seasonal variation, but lets keep it simple here.\nTo fit this model, we must estimate multiple parameters for every timestep and every pixel in the landscape on a 250m grid. That ends up at 1440000 * n parameters * t timesteps for the 90 000 km2 of the CFR. As you can imagine the model takes a VERY long time to fit using MCMC (around 2 weeks).\nMCMC\nTo get a feel for this problem I will fit the model using MCMC to a small sample of sites from across the region using Stan and R.\nLets load up the libraries we need\n\n\nlibrary(cmdstanr)\nlibrary(posterior)\nlibrary(bayesplot)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(knitr)\nlibrary(ggplot2)\ncolor_scheme_set(\"brightblue\")\nset.seed(83)\n\nand pull in the data for some plots\n\n\n#read raw data\npostfire <- read.csv(\"postfire.csv\")\n#tidy up\npostfire <- postfire %>%\n  filter(ND>0) %>% #remove impossible NDVI values\n  filter(nid %in% as.numeric(sample(levels(as.factor(postfire$nid)),50))) %>% #subset plots\n  mutate(age=DA/365.23) %>% #convert age from days to years\n  select(age=age,nd=ND,pid=nid,env1=map,env2=tmax01) %>%\n  mutate(pid = as.numeric(as.factor(pid))) %>%\n  mutate(env1 = (env1 - mean(env1))/sd(env1),env2 = (env2 - mean(env2))/sd(env2)) #standardize env variables\n\n#plots level data on env conditions\npostfire_grp <- postfire %>%\n  group_by(pid) %>%\n  summarise(envg1 = max(env1),envg2 = max(env2))\n\n#prep data for stan\npostfire_data <- list(N = nrow(postfire),\n                     J= nrow(postfire_grp),\n                     nd=postfire$nd,\n                     age= postfire$age,\n                     pid=postfire$pid,\n                     envg1 = postfire_grp$envg1,\n                     envg2 = postfire_grp$envg2)\n\nkable(head(postfire))\nage\nnd\npid\nenv1\nenv2\n14.04594\n0.4462\n1\n0.5622957\n-0.8759055\n14.08975\n0.4667\n1\n0.5622957\n-0.8759055\n14.13356\n0.4806\n1\n0.5622957\n-0.8759055\n14.17737\n0.4789\n1\n0.5622957\n-0.8759055\n14.22118\n0.4582\n1\n0.5622957\n-0.8759055\n14.26498\n0.4628\n1\n0.5622957\n-0.8759055\n\nWe have age in years, a plot identifier pid. the observed ndvi nd and two plot level environmental variable env1, which is mean annual precipitation, and env2, which is the summer maximum temperature.\nLets load up our Stan model which codes the model described above. This is not a particularly clever or efficient way of coding the model, but it is nice and readable and works fine on this example dataset\n\n\nmodel <- cmdstan_model('firemodel_predict.stan', compile = TRUE)\nmodel$print()\n\ndata {\n  int<lower=0> N;\n  int<lower=0> J;\n  int<lower=1,upper=J> pid[N];\n  vector[N] age;\n  vector[N] nd;\n  vector[J] envg1;\n  vector[J] envg2;\n  \n}\nparameters {\n  vector[J] alpha;\n  vector[J] gamma;\n  vector[J] lambda;\n  real alpha_mu;\n  real gamma_b1;\n  real gamma_b2;\n  real lambda_b1;\n  real lambda_b2;\n  real<lower=0> tau_sq;\n  real<lower=0> gamma_tau_sq;\n  real<lower=0> lambda_tau_sq;\n  real<lower=0> alpha_tau_sq;\n}\n\ntransformed parameters {\n  vector[N] mu;\n  vector[J] gamma_mu;\n  vector[J] lambda_mu;\n  real tau = sqrt(tau_sq);\n  real gamma_tau = sqrt(gamma_tau_sq);\n  real lambda_tau = sqrt(lambda_tau_sq);\n  real alpha_tau = sqrt(alpha_tau_sq);\n  \n  for (j in 1:J){\n    gamma_mu[j] = (envg1[j]*gamma_b1) + (envg2[j]*gamma_b2);\n    lambda_mu[j] = (envg1[j]*lambda_b1) + (envg2[j]*lambda_b2);\n  }\n  \n  for (i in 1:N){\n    mu[i] = exp(alpha[pid[i]])+exp(gamma[pid[i]])-exp(gamma[pid[i]])*exp(-(age[i]/exp(lambda[pid[i]])));\n  }\n}\n\nmodel {\n\n  tau ~  inv_gamma(0.01, 0.01);\n  gamma_tau ~ inv_gamma(0.01, 0.01);\n  lambda_tau ~ inv_gamma(0.01, 0.01);\n  alpha_tau ~ inv_gamma(0.01, 0.01);\n  \n  alpha_mu ~ normal(0.15,3);\n  gamma_b1 ~ normal(0,3);\n  gamma_b2 ~ normal(0,3);\n  lambda_b1 ~ normal(0,3);\n  lambda_b2 ~ normal(0,3);\n  \n\n  alpha ~ normal(alpha_mu, alpha_tau);\n  gamma ~ normal(gamma_mu,gamma_tau); \n  lambda ~ normal(lambda_mu,lambda_tau); \n\n  nd ~ normal(mu, tau);\n  \n}\n\ngenerated quantities {\n  vector[N] nd_new;\n  \n  for (n in 1:N){\n    nd_new[n] = normal_rng(mu[n], tau);\n  }\n}\n\nWe then fit the model\n\n\nfit_mcmc <- model$sample(\n  data = postfire_data,\n  seed = 123,\n  refresh=500,\n  chains = 2,\n  parallel_chains = 2,\n  iter_warmup = 1000,\n  iter_sampling = 2000\n)\n\n\n\n\nand do the normal checks to see that our MCMC sampler behaved itself (skipped here). If we check our posterior traces for some parameters they looks like healthy, hairy caterpillars\n\n\nmcmc_trace(fit_mcmc$draws(c(\"tau\",\"alpha_mu\")))\n\n\nThe crux here is how long it takes to run this sampler\n\n\nfit_mcmc$time()$total\n\n[1] 2954.735\n\n2954 seconds or 50 minutes to do just 50 sites. Remember the full dataset is 1440000 sites. While there are a couple of good options to improve the efficiency of this model, and neat new ways of parallelizing within-chain operations in Stan, MCMC is always going to take a long time for this kind of task. Wouldn’t it be great if there was another, faster way of estimating the full posterior….\nVariational Inference\nVariational inference (VI) is a method for approximating the posterior that has been around for a while. VI is much, much faster than MCMC, and can scale to very large datasets. The major drawback that has limited it’s use is that model-specific calculations and derivations are required to implement VI. The joke is that deriving your VI algorithm is what you do while you wait for your MCMC sampler to finish. But there has been some exciting activity in VI research lately that has made it possible to automate the process of deriving scalable variational inference algorithms for a wide class of models. I am going to skip the details here, but these publications are worth digging into if you are interested in this (Kucukelbir et al. 2015, 2017; Blei, Kucukelbir, and McAuliffe 2017).\nThe salient point is that Automatic Differentiation Variational Inference (ADVI) turns the task of computing a posterior into an optimization problem - we simply need to use an optimization algorithm like stochastic gradient ascent (SGA) to maximize a loss function. With SGA we can also feed our data in as mini-batches (subsamples), which adds the potential to scale to very large datasets. The best part of all of this is that ADVI is already implemented in a number of probabilistic programming languages, including Stan. All we need is to change one line of code and we can fit the same Stan model using ADVI:\n\n\nfit_vb <- model$variational(\n  data = postfire_data,\n  adapt_engaged=FALSE,\n  eta=0.1,\n  tol_rel_obj = 0.001,\n  seed = 123)\n\n\n\n\nHow long did that take?\n\n\nfit_vb$time()$total\n\n[1] 28.3975\n\n28 seconds, or about 100 times faster than MCMC. In separate tests I have tried it doesn’t seem to take much longer on the full dataset.\nOk, what’s the catch?\nADVI, like all of variational inference, is an approximate inference technique; whereas MCMC, if done right, will converge to the exact posterior. ADVI assumes no dependency (i.e. zero correlations) between parameter distributions, if this is not the case, ADVI can underestimate variances in the parameter distributions (though there are extensions that can help with this).\nIn practice what this means that if you want to use ADVI it is a good idea to compare posteriors estimated using ADVI and MCMC, if this is possible. For our model the ADVI and MCMC parameter estimates are pretty close\n\n\nbayesplot_grid(\n  mcmc_dens(fit_vb$draws(\"tau\")),\n  mcmc_dens(fit_mcmc$draws(\"tau\")),\n  titles = c(\"MCMC\", \"ADVI\"),\n  xlim = c(0.04, 0.07)\n)\n\n\n\n\nbayesplot_grid(\n  mcmc_dens(fit_vb$draws(\"alpha_mu\")),\n  mcmc_dens(fit_mcmc$draws(\"alpha_mu\")),\n  titles = c(\"MCMC\", \"ADVI\"),\n  xlim = c(-2.5, 0)\n)\n\n\n\n\nbayesplot_grid(\n  mcmc_dens(fit_vb$draws(c(\"gamma_b2\",\"gamma_b1\",\"lambda_b1\",\"lambda_b2\"))),\n  mcmc_dens(fit_mcmc$draws(c(\"gamma_b2\",\"gamma_b1\",\"lambda_b1\",\"lambda_b2\"))),\n  titles = c(\"MCMC\", \"ADVI\"),\n  xlim = c(-1, 1),\n  grid_args=list(ncol=2)\n)\n\n\nBut what we are really interested in with this model is the posterior predictions of observed NDVI. We want to use the model to estimate future NDVI and compare to observations. The idea is that this comparison will allow us to diagnose sites where NDVI is going off-track, and there may be some underlying ecological problem (see (Slingsby, Moncrieff, and Wilson 2020) for more details). Even if ADVI is not exact, if it returns posterior predictive intervals very close to MCMC ina fraction of the time it would mean we could scale this idea to very large areas.\nLets try this on our data. First lets extract the posterior predictive intervals that we generated in the MCMC and ADVI model and wrangle them into a neat data frame\n\n\n\n\n\n#posterior predictive\nstan_vb <- fit_vb$summary(\"nd_new\",\"mean\",\"quantile2\")\nstan_mcmc <- fit_mcmc$summary(\"nd_new\",\"mean\",\"quantile2\")\n#add observed data\nstan_vb$method <- \"ADVI\"\nstan_vb$age <- postfire_data$age\nstan_vb$pid <- postfire_data$pid\nstan_vb$nd <- postfire_data$nd\nstan_mcmc$method <- \"MCMC\"\nstan_mcmc$age <- postfire_data$age\nstan_mcmc$pid <- postfire_data$pid\nstan_mcmc$nd <- postfire_data$nd\n#wrangle\nstan_ndvi <- rbind(stan_vb,stan_mcmc) %>%\n  select(age=age,NDVI=nd,pid,mean,upper=q95,lower=q5,method) %>%\n  filter(pid %in% as.numeric(sample(levels(as.factor(stan_vb$pid)),6)))\nstan_ndvi$method <- ordered(stan_ndvi$method)\n\nWhen we make this comparison, the posterior predictive intervals from ADVI and MCMC are almost identical\n\n\nggplot(data=stan_ndvi,aes(x=age)) +\n  geom_line(aes(y=mean,lty=method),colour=\"blue\") +\n  geom_line(aes(y=NDVI),colour=\"black\",lwd=0.5,alpha=0.3) +\n  geom_ribbon(aes(ymin=lower,ymax=upper,fill=method),alpha=0.5)+\n  facet_wrap(~pid) +\n  xlim(c(0,20))+\n  labs(x=\"time since fire (years)\",y=\"NDVI\") +\n  theme_bw()\n\n\nThis is great news. If we were trying to make inferences about ecological process from parameters estimates, we would want an exact method like MCMC (Wilson, Latimer, and Silander 2015). But for the purposes of monitoring vegetation we are not interested in this - we only want a method that give us a good-enough fit and that is practical to deploy. ADVI fits the bill here, and we are excited to see where we go with the newfound ability to scale our analyses to bigger areas and finer scales.\n\n\nBlei, David M., Alp Kucukelbir, and Jon D. McAuliffe. 2017. “Variational Inference: A Review for Statisticians.” Journal of the American Statistical Association 112 (518): 859–77.\n\n\nKucukelbir, Alp, Rajesh Ranganath, Andrew Gelman, and David M. Blei. 2015. “Automatic Variational Inference in Stan.” arXiv:1506.03431 [Stat], June. http://arxiv.org/abs/1506.03431.\n\n\nKucukelbir, Alp, Dustin Tran, Rajesh Ranganath, Andrew Gelman, and David M. Blei. 2017. “Automatic Differentiation Variational Inference.” The Journal of Machine Learning Research 18 (1): 430–74.\n\n\nSlingsby, Jasper A., Glenn R. Moncrieff, and Adam M. Wilson. 2020. “Near-Real Time Forecasting and Change Detection for an Open Ecosystem with Complex Natural Dynamics.” ISPRS Journal of Photogrammetry and Remote Sensing 166 (August): 15–25. https://doi.org/10.1016/j.isprsjprs.2020.05.017.\n\n\nWilson, Adam M, Andrew M Latimer, and John A Jr Silander. 2015. “Climatic Controls on Ecosystem Resilience: Postfire Regeneration in the Cape Floristic Region of South Africa.” Proceedings of the National Academy of Sciences of the United States of America 112 (29): 9058–63. https://doi.org/10.1073/pnas.1416710112.\n\n\n\n\n",
    "preview": "posts/2020-08-12-advi-vs-mcmc/mcmc_vs_advi.png",
    "last_modified": "2022-10-07T11:52:35+02:00",
    "input_file": {}
  },
  {
    "path": "posts/repro-r-1/",
    "title": "Reproducible R part 1",
    "description": "A gentle introduction to reproducible computing environments and research compendia",
    "author": [
      {
        "name": "Glenn Moncrieff",
        "url": {}
      }
    ],
    "date": "2020-04-16",
    "categories": [
      "R tutorial",
      "Reproducible science"
    ],
    "contents": "\nYou will get the most benefit from this tutorial if you already use git and github in your workflow. These are essential tools for reproducibility in science. If you are not familiar with them check out this tutorial by R guru Jenny Bryan for a great comprehensive introduction, or this tutorial for a crash course\nAnybody who works with data has at some point heard a colleague say ‘Well, it works on my computer’, expressing dismay at the fact that you cannot reproduce their results. \nOne of the principle motivations for using software like R or Python to script our analysis over a point-and-click approach is to make our work reproducible -; increasing both the ease of collaboration and transparency in science. Even if we use free and open-source software, and even if we write clean, well documented code, and even if we share all our data using best practices it is very rare for the software on any two computers to be identical. When you are trying to run someone else’s code, it is likely that one of the following things will differ between your machine, and the machine the code was tested/developed on:\nThe version of the software used (eg R 3.6.1 vs R 3.5.2)\nThe versions of the packages/extensions used (ggplot2 3.3.0 vs ggplot2 2.2.0)\nExternal software dependencies for packages (eg gdal for spatial analysis)\nWe often refer to these three things collectively as your environment. Often your code will run without any hiccups even when environments differ. Software engineers try to make sure your code will not stop working if you use a slightly different versions. But with all the software and all the dependencies eventually something will give and things will break. Even if the code runs successfully, there is a chance that the results could differ.\nThe only way to be certain that code written by someones else will run on your machine and will produce the same results is to replicate their environment\nIt would be impossible to do this manually, and so there are plenty of solutions that range from the simple (ensuring you are using the same package versions), to the complex (almost reproducing your entire computer). We will start by exploring the easy-to-sue solution developed for R by our benevolent overlords at RStudio: renv\nrenv\nrenv isolates your project package dependencies ensuring that you have a record of exactly what packages and which version you used. Normally when you install or update a package it is installed into a global library of packages on your machine (you can see the location of installed packages if you run .libPaths()). You will then load the packages used in your project in each script as and when you need. There is no record of which package and which version you used other than you calls to library(). We can use renv to create a file that records exactly that. This file is called the lockfile. Here is an example of what the lockfile looks like inside\n\n\"R\": {\n    \"Version\": \"3.6.2\",\n    \"Repositories\": [\n      {\n        \"Name\": \"CRAN\",\n        \"URL\": \"https://cran.rstudio.com\"\n      }\n    ]\n  },\n\"Packages\": {\n    \"markdown\": {\n      \"Package\": \"markdown\",\n      \"Version\": \"1.0\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"CRAN\",\n      \"Hash\": \"4584a57f565dd7987d59dda3a02cfb41\"\n    },\n    \"mime\": {\n      \"Package\": \"mime\",\n      \"Version\": \"0.7\",\n      \"Source\": \"Repository\",\n      \"Repository\": \"CRAN\",\n      \"Hash\": \"908d95ccbfd1dd274073ef07a7c93934\"\n    }\n  }\n}\nUsing renv\nUsing renv and creating lockfiles is very easy, requiring little to no modification to your existing workflows. Everything happens inside an R project, so make sure you create a separate project for each analysis for which you want to make a reproducible environment. You will also need to install renv before starting\n\n\ninstall.packages('renv')\n\nWhen you are ready to start working, run\n\n\nrenv::init()\n\nthis will initialize a new environment specific to your current project. You can now work as you normally would: writing code, installing packages, and loading them as and when you need. When you are ready to take a snapshot of your current environment run\n\n\nrenv::snapshot()\n\nto save the state of the project environment to the lockfile (renv.lock) in your project home directory. This details of which packages you used and their versions are now recorded to ensure that you environment is reproducible. You can continue working now and make further changes, and update the lockfile by running renv::snapshot() again.\nPerhaps you have made changes to your environment (eg updated a package that breaks your code). You can return to your previous state saved in the lockfile by running\n\n\nrenv::restore()\n\nthis will return to the packages and version the last time you called renv::snapshot().\nBut what if you already have a project and you want to start using renv to manage the code and packages. No problem. Simply open the project and run renv::init(). The newly-created project environment search for any packages loaded in R files within the project, install them to the local package library, when you call renv::snapshot() these packages will be included in the lockfile.\n\nTip: run all renv:: commands directly in the R console. There is no need to save them in your scripts because they only need to be run once, or infrequently, per project.\n\nInstalling and managing packages\nWhen you initialize a new environment with renv::init renv will search for packages mentioned in .r files in your project and install them in your new project environment, if you run renv::init at the start of a project before writing any code, you start nothing but the default r packages. You will need to install each package needed using install.packages. Only those packages loaded in your R scripts (and their dependencies) will be included in the lockfile. This is not ideal because it can take a while to install some packages, and will be come painful if you have waste this time every time you start a new R project. But renv has a solution to this. The full new package is only downloaded and installed the first time you install it after calling renv::init. In subsequent projects when you install the package renv will simply create a copy of the already installed package.\nTry it yourself\nNow is a good time to setup you first renv project. Open a new project in Rstudio. Run script to create a bog standard ggplot, then save it\n\n\nlibrary(ggplot2)\n\nggplot(data=iris,aes(x=Sepal.Length,y=Petal.Length,colour=Species)) +\n  geom_point()\n\n\nNow run renv::init in the console. This will initialize your project, and install the libraries that are loaded by your scripts. You will see your project file .Rproj, your file with r code .r, a folder created by renv called renv which contains all the libraries for your project, and the lockfile renv.lock. Now you could continue working in your project taking snapshots with renv::snapshot or restoring to previous snapshots with renv::restore as and when you need.\nCollaborating and sharing environments\nWhile it is great to use renv to ensure that our code will always run regardless of changes we have made to our R installation or packages, the real motivation for adopting renv is to facilitate better collaboration. By creating and sharing lockfiles along with project code, collaborators (and our future selves - our most important collaborator) are just a renv::restore away from being able to run our code.\nThe best part of this is how little is required. Simply share you lockfile, and whomever is using your code simply needs to run renv::init and renv::restore in a project with your code and lockfile. Assuming you are using git and github to share code, this means simply adding the lockfile to your github repo. Lets try this ourselves.\nThe data\nFor rest of this tutorial we will continue to work with an example dataset to demonstrate various approaches for reproducible R. The dataset is from a set of experiments run in South Africa between 1930 and 1990, testing the impact of alien tree species grown in plantation forests on streamflow in unmodified river catchments. The flow of rivers in each planted catchment was monitored over a number of years and compared to the flow in an adjacent unplanted river. The amount that flow was reduced in the planted catchment compared to the unplanted catchment is reported as a percentage and recorded every year as the plantation aged.\n<img src=\"/weir.jpeg>\nWe would like understand how the streamflow reduction caused by the plantation increases as the plantation gets older and the impact of planting different species. This is what the data looks like\n\nage\nspp\ncondition\nlocation\nflow_reduction\n0\neuc\nopt\nwest_d\n0.1638554\n1\neuc\nopt\nwest_d\n0.0578313\n2\neuc\nopt\nwest_d\n0.2867470\n3\neuc\nopt\nwest_d\n0.7421687\n4\neuc\nopt\nwest_d\n0.8012048\n5\neuc\nopt\nwest_d\n0.6397590\n\nThe full report on these experiment, from which these data were transcribed is available here\nEach row in the data represents the measured streamflow reduction in a particular catchment at a particular age\nColumn age is time in years since the catchment was planted\nspp is the forestry species, with euc = Eucalyptus spp and pin = Pinus spp\ncondition indicates the suitability of that location for growing the planted tree species, coded as either opt for optimal, or sub for suboptimal\nlocation encodes the site of the plantation, and finally\nflow reduciton, our response variable, indicates the percent reduction in streamflow relative to a reference catchment\nClone the code and data\nTo get started, we begin as we often would, by cloning a github repository to our machine. We could do this the old-fashioned way, using git clone in the terminal, or clicking New project and selecting Version control in the Rstudio GUI. A neater way to do this is to use the package usethis which has lots of helpful functions for package developments and reproducibility in r. Running this code in R will clone the repo into a new project\n\n\n\nbecause a lockfile is included in the repo, all you need to do is run renv::init to initialize your environment and renv::restore to bring the required packages into your environment and you are ready to run the code. Try it yourself and look at the dodgy analysis and it’s result\n\n\n\n\n\n\nyou could improve on this, perhaps add some new analysis requiring additional packages. Then take a snapshot renv::snapshot and push the changes to github\nFull reproducibility\nrenv is sufficient for most of our needs, ensuring that we and our collaborators are working with identical R environments. What it does not do is ensure that everything else behind the scenes on our machines are identical. This is not an issue most of the time because whether your code runs on a mac or a linux pc will not often alter the results of your R analysis. However, there are some bases that renv does not cover\nSome R packages require system dependencies, and renv may struggle to set them up for you\nSome people do not want to have to download all your code and install all the packages to explore your analysis\nSome people don’t use R or have it installed on their machine, but may still want to interact with and interrogate your analysis\nWouldn’t it be great if there was someway to have a virtual machine available online to anybody that has all the packages installed needed to perform your analysis? With the simple click of a button they could fire up the machine, which would already have all your code and data, and run your code to inspect your results. Well…\nBinder\nThat is exactly what Binder does. You give binder the address of the git repo with your code and data, making sure that it has a file like a lockfile describing your environment, and binder will fire up a virtual machine running RStudio with everything needed to run your analysis already installed. Binder machines are small and should not be used for intensive computation. But they are really well suited for reproducing papers/figures/examples.\nA typical workflow would involve having all your code in a github repo, including a file called runtime.txt specifying the version of R used and install.R which lists the packages you need (Binder has not caught up with lockfiles yet). Then all you need to do is copy and paste the link to the github repo into the form on www.mybinder.org. It will churn away for a short while and eventually produce a link that takes you to a fully fledged RStudio instance accessible to anyone through their browser, with all the contents of your github repo inside. Lucky for us, even that simple step is not necessary, as there is a neat R package that will automate this for us.\nHolepunch\nHolepunch automates the process a creating a binder instance from your R project. Lets go back to our streamflow project to see how it works. To install holepunch\n\n\ndevtools::install_github(\"karthik/holepunch\")\n\nTo use holepunch we need to have our analysis setup as an R project and linked to a github repo. Fortunately this is something we always do if we want to make our work reproducible so it is no extra work, and has already been done in our streamflow project. The first step is to create the two files needed by binder\n\n\nlibrary(holepunch)\n#install.R\nwrite_install()\n#runtime.txt\nwrite_runtime()\n\nnext we add a link on the readme of our github readme to the binder instance. This link will look look this <img src=\"launch.png>\n\n\ngenerate_badge()\n\nNow is the time that we want to push these updates to our github repo. Now if somebody wants to launch our binder they just have to click on the badge on the github page. This will take a while the first time you run it, as it needs to build an entire machine with all the software needed. Subsequently it will be much faster, as the machine image will already exist. Once this completes successfully you can click on the badge in your github repo and behold.\nTry it out \nResearch compendia\nUp until now we have limited the discussion to ensuring our computing environment is reproducible. This is a very important component of reproducibility, but it is not sufficient. True reproducibility requires a computing environment, but it also requires code, well documented data, a description of how the data were collected and the analysis performed and a discussion of the results (a research paper essentially). The concept used to describe this complete research package is the _research compendium__. Marwick et al describe this idea in their paper titled Packaging data analytical work reproducibly using R (and friends).\nThe components of a complete research compendium can be quiet intricate: .r files with the analysis code, .Rmd files with the research paper, a DESCRIPTION or .lock file describing the environment, along with data in a /data subfolder. Your project folder might look something like this (lifted form the Marwick paper)\n\nThis is the gold standard for reproducibility. If you have a well structured, and well documented research compendium you have earned you black belt in scientific integrity. It may seem like an arduous process to set this up. But with the help of holepunch it is a simple matter of running a few functions. You do need to take it upon yourself to write legible code, and to structure your project well with separate folders for data, code etc.\nThe first and most important part of creating your compendium is the DESCRIPTION file. This file give all the important details of the project eg the author, the purpose of the project, the license and all the package dependencies needed to recreate the environment. Here is a simple example\n\nType: Compendium\nPackage: South Africa Streamflow\nTitle: Curve fitting to Paired Catchment Data\nVersion: 1.0\nAuthors@R: \n    person(given = \"Glenn\",\n           family = \"Moncrieff\",\n           role = c(\"aut\", \"cre\"),\n           email = \"glenn@saeon.ac.za\",\n           comment = c(ORCID = \"0000-0002-0066-4371\"))\nDescription: A re-analysis of the South African paired catchment data\nLicense: MIT\nDepends: \n    betareg,\n    broom,\n    devtools,\n    dplyr,\n    ggplot2,\n    holepunch,\n    knitr,\n    readr,\n    renv,\n    rmarkdown,\n    usethis\nEncoding: UTF-8\nLazyData: true\nCreate a resaerch compendium with holepunch\nTo create a DESCIPTION file with holepunch\n\n\nwrite_compendium_description(\n  type = \"Compendium\",\n  package = \"South Africa Streamflow\",\n  description = \"A re-analysis of the South African paired catchment data\",\n  version = \"1.0\")\n\nBefore running this though it can be useful to set some options regarding author details that will automatically be written to the DESCIPTION file. This will reduce the amount of manual editing we will need to do to this file\n\n\noptions(\n  usethis.full_name = \"Glenn Moncrieff\",\n  usethis.description = list(`Authors@R` = 'person(\"Glenn\", \"Moncrieff\", email = \"glenn@saeon.ac.za\", role = c(\"aut\", \"cre\"), comment = c(ORCID = \"0000-0002-0066-4371\"))',License = \"MIT\")\n  )\n\nNow we are ready to create a binder image if we like. Rather than create the runtime.txt and install.R files, we create something called a Dockerfile. The Dockerfile contains the full instructions on how to create the machine on which to run our code (the runtime.txt and install.R are actually just used by binder to create the Dockerfile). To create the Dockerfile with holepunch\n\n\nwrite_dockerfile(maintainer = \"Glenn Moncrieff\") \n\nLike before, lets add a nice badge that will launch the binder repo directly from our github repo\n\n\ngenerate_badge() \n\nNow we are ready to push our complete research compendium to github. Now somebody interested in running our analysis can just click the binder badge and explore the data.\nAt this point you might want to publish your research compendium to a site like figshare or zenodo. You should archive a specific commit of your compendium that corresponds to a submitted or published manuscript. Your archive will have a DOI associated with it. This means you have a publicly available snapshot of the code that matches the paper. Code development can continue after the paper is published, but with a DOI that links to a specific commit, other users of the code can be confident that they have the version that matches the paper. A DOI also simplifies citation of the compendium, so you can cite it in your paper (and others can cite it in their work) using a persistent URL.\n\nSource material:\nropensci/rrrpkgIntroduction to renv\n\n\n",
    "preview": "posts/repro-r-1/2020-04-16-reproducible-r-part-1_files/figure-html5/unnamed-chunk-9-1.png",
    "last_modified": "2022-10-07T11:52:35+02:00",
    "input_file": {}
  },
  {
    "path": "posts/wiped-off-the-map/",
    "title": "Wiped off the map: historical aerial photos document the communities Apartheid tried to erase",
    "description": "Interactive maps to visualize and explore apartheid removals in three communities in Cape Town",
    "author": [
      {
        "name": "Glenn Moncrieff",
        "url": {}
      }
    ],
    "date": "2019-09-24",
    "categories": [
      "Geospatial"
    ],
    "contents": "\n\n\n\nSince the 1930s aerial photographs covering all of South Africa have been collected, documenting radical changes in our country’s landscape. Incredible stories are documented in this archive. They show how human activities have encroached on natural ecosystems, how cities have grown and reshaped themselves over time and in some cases, how apartheid left scars that remain unhealed to this day. Below are three interactive maps showing the brutal reality of how three communities were erased by the apartheid government. As you move the slider, you will move between two images; one of a thriving community taken in 1945, and another of the area as it appears today. These images give an indication of the scale of the destruction, but they do not tell the human side of this story. I am not qualified to even attempt telling these stories, and therefore leave this unmentioned. Rather, I will add links to where more can be found out.\nDistrict Six\nsource: South African history onlineRichmond Street in District Six before forced removal of the community\n\n“District Six is a blot which the government has cleaned up and will continue to clear up.” - P.W Botha\n\nDistrict Six is the best known example of forced removal in Cape Town. It was a thriving inner city community of around 60 000 people. On 11 February 1966 the apartheid government declared District Six a whites-only area under the Group Areas Act. In the following years the entire community was relocated to the Cape Flats. Home and businesses were demolished, with only a few structures - such as places of worship - escaping the devastation. Attempts to redevelop the land stalled due to condemnation from the international community and uncertainty over potential land restitution. Aside from the construction of the Cape Peninsula University of Technology, much of the land remains undeveloped today. The land restitution process is ongoing, with many claimants upset at the long wait to return home.\n\n\n\nThe District Six museum tells the story of this community in it’s own words.\nDistrict six may be the best known example, but all over Cape Town non-white residents in areas declared white-only under the group-areas act were being evicted from homes where they had lived for generations. Many of these evictions are not visible in the historical aerial photos, but in the case where entire communities where removed and their homes demolished, the evidence is clear and striking.\nProtea village, Bishopscourt\nNeighboring Kirstenbosch National Botanical gardens, what is today known as Boschendal Arboretum in the wealthy suburb of Bishopscourt was previously known as Protea village. When it was demolished at the end of the 1960s, the village had around 600 inhabitants. Many residents were employed at Kirstenbosch or as domestic workers in wealthy households. Almost all the homes and the school in Protea village were destroyed, but the Church of the Good Shepherd and three stone cottages remain today.\n\n\n\nIn 2006, under the government’s land restitution program, former residents were given back 12.5 hectares of land to which 86 families were planning to return. Anna Bohlin has written about the history of this community and the reaction to their return home.\nThe Church of the Good ShepherdConstantia\nThe suburb of Constantia is one of the most upmarket areas of Cape Town, with large houses bordering wine farms and lush green rivers. Before apartheid forced removals, the majority of residents were non-white, with many families farming their own land. During apartheid, non-white land-owners were forced to sell their land, severing the connection that mutligenerational families had with the land and hastening the demise of Constantia’s agrarian landscape. The Solomon family, who farmed the land between Ladies Mile and Spaanschemat river road from 1902, occupied their farm for 65 years before they were removed under the Group Areas Act. The Solomon family initiated the restitution process in 1996 and, 10 years later, the Land Claims Court concluded a deed of settlement with the Solomon Family.\n\n\n\n\n\n",
    "preview": "posts/wiped-off-the-map/wiped.png",
    "last_modified": "2022-10-07T11:52:35+02:00",
    "input_file": {}
  },
  {
    "path": "posts/thicket_change/",
    "title": "Real-time detection of land cover change",
    "description": "Detecting vegetation clearing using Sentinel 2 time series",
    "author": [
      {
        "name": "Glenn Moncrieff",
        "url": {}
      }
    ],
    "date": "2018-10-31",
    "categories": [
      "Geospatial"
    ],
    "contents": "\nIf you were to judge by media coverage climate change and poaching would probably rank as the biggest threats to biodiversity globally. Yet when systematically assessed, the most important factor causing the massive declines in species across all ecosystems both terrestrial and aquatic is habitat transformation. In South Africa’s terrestrial ecosystem the same is true. High rates of irreversible change to natural ecosystems is ongoing and loss of natural habitat continues to be the leading threat to terrestrial biodiversity.\nA range of tools are available to aid in mapping and monitoring land cover change. Global Forest Watch is an amazing tool developed by the World Resources Institute for monitoring deforestation worldwide. They continuously update maps of where deforestation is occurring based upon data collected by the USGS/NASA satellite Landsat. While this is a great tool, it is not designed with South Africa’s ecosystems in mind, and thus has limited applicability. South Africa has a national land cover map tracking the distribution of a range of natural and non-natural land cover types based upon the same Landsat data as Global Forest Watch. The problem with the national land cover map is that is is only updated every 5-10 years on a somewhat ad hoc basis. It allows us to report on the rate at which habitats are being destroyed, but neither it nor Global Forest Watch (which provides updates every year) produce information fast enough to inform interventions that could halt habitat destruction while it is happening. Enforcement authorities are in particular need of information that might help them to catch land owners in the act of illegally transforming natural vegetation, as currently only 5% of investigated cases end in a successful prosecution.\nNewly launched satellite platforms have reduced the time between image acquisitions and increased image resolution to the point that it is now possible to detect land transformation within days of its occurrence. Sentinel 2 - launched by the European Space Agency (ESA) on the 23rd of June 2015 consists of 2 satellites capable of revisiting any place on earth every 5 days and producing images at 10 meter spatial resolution. Even more detail is available from the constellation of almost 200 microsatellites owned by Planet Inc. Planet’s satellites reimage the Earth every day at 3 meter resolution. ESA’s open data policy makes it possible to monitor large areas using their data as it comes at no cost, while Planet provides a limited amount of data for free to academic users allowing focused monitoring over key areas.\nI set out to test how rapidly land transformation could be detected using these two platforms in an area of South Africa that is undergoing rapid irreversible land cover change. The Albany Thicket biome is a part of a global biodiversity hotspot and home to an amazing array of indigenous plants and animals - many of which are of immense economic, cultural and spiritual value to local people.\nAlbany ThicketEnforcement authorities have reported high rates of illegal vegetation clearing across the biome, but an area of particular concern surrounds the town of Alexandria near Port Elizabeth, where patches of thicket and forest undisturbed for thousands of years are being cleared for diary pasture at an alarming rate. I identified a small area where a patch of vegetation had recently been cleared and set myself the task of attempting to automate the detection of this change within days of its occurrence. Here you can see a timelapse covering the period from the 23rd of January to the 2nd of February 2018. From this imagery is is apparent that the vegetation was cleared around the 1st of February. (Disclaimer: I make no allegations about the legality of this vegetation clearing. It is possible that what this landowner did is entirely legal)\nthicket clearingAll the code to reproduce my analysis is available at this github repo. But here is a quick breakdown of how I went about this:\nFirst I downloaded all the data available for this area from Planet and Sentinel 2 from when their records began until the 31st of March 2018 - about 2 months after the clearing event. This amounts to about 2 years of data for each. In the end I had around 120 images to analyse from Planet and 50 from Sentinel 2. Then I needed to correct the raw data from level 1 data, or what the sensor sees at the top of the atmosphere, to surface reflectance - what the earth’s surface looks like without the interference of the atmosphere. This is done by correcting the data for the effect of things like clouds, haze, the varying angle and intensity of the sun’s illumination, and topography. Fortunately Planet have done this for me already and I can simply download their surface reflectance data. Sentinel provide a tool Sen2Cor which you can run on level 1 imagery to calculate surface reflectance. Using the processed surface reflectance data I calculate a measure of vegetation greeness called NDVI or Normalized Difference Vegetation Index. NDVI is a measure of how much photosynthetic activity there is. A high NDVI value (near 1) indicates lush vegetation like a forest, while a low value (around 0.2) indicates bare soil. A sudden drop in NDVI suggests a reduction in photosynthesis, and may be related to vegetation loss. After downloading all the data, processing it to surface reflectance and calculating NDVI I end up with a stack of NDVI images for a series of dates across the region I wish to monitor.\nndviThese data are the basis upon which we detect land transformation using an algorithm called BFAST (Breaks For Additive Season and Trend). BFAST works by defining in a period in which we know vegetation to be stable and not subject to transformation. The trends and behaviour of NDVI through time within this period are used to build of model of what the expected pattern ought to look like if this vegetation where to continue to function similarly. We then project expected NDVI patterns into the future and compare these to new data as they are acquired. If the new data exceeds our expectations by some threshold we flag this as an anomaly and a potential case of land cover change.\nbfastThis is the procedure that was followed using the Sentinel 2 and Planet data. We build models using data up to the 31st of December 2018 and monitored change from the 1st of January 2018 onwards. Using the Planet data I was able to detect the land transformation we were interested in within 1 to 2 days of its occurrence! The breakpoint is detected on day 33 of the year - or the 2nd of Feb. Remember the raw images showed the change occurring on the 1st. This is what the actual data look like:\nbfast_planetRunning this over the whole area of interest shows how we can accurately outline the clearing and automate the detection of areas that are cleared. The timelapse shows when the detection occurs by highlighting the cleared area - a day after it occurs!.\nthicket clearing2This process is not perfect. You can see how we outline some areas that have not been transformed, and similarly, it is likely that we will fail to detect some areas that have been cleared.\nIt takes around a month after clearing to detect the event using Sentinel data, but it also provides a good outline of areas that have changed during the monitoring period. By the 13th of March clearing had been accurately detected in the areas we are concerned with, and an additional area to the north that I had not been previously aware of\n\n\nI think this is really promising. Satellite imagery has advanced enormously in the last few years. We are now at a point where we have the imagery available to identify land cover change within days of its occurrence. Of course it is not possible to manually look through images and search for change each day, thus we need algorithms like those I have demonstrated to help with this process. I don’t think we can fully automate this, there will always be the need for humans in the loop to confirm detection and throw away false positives. We have some grants that are currently being reviewed in which we propose to further develop this technology and roll it out over large areas. If things come together you will see this approach in a thicket near you soon.\n\n\n",
    "preview": "posts/thicket_change/cover.png",
    "last_modified": "2022-10-07T11:52:35+02:00",
    "input_file": {}
  },
  {
    "path": "posts/fynbos_id/",
    "title": "Automated fynbos identification using iNaturalist and Deep Learning ",
    "description": "Identifying Restios and Proteas using Tensorflow and transfer learning",
    "author": [
      {
        "name": "Glenn Moncrieff",
        "url": {}
      }
    ],
    "date": "2018-03-10",
    "categories": [
      "R tutorial",
      "Python tutorial",
      "Neural Networks"
    ],
    "contents": "\nA lack of knowledge of neural networks has probably been one of the biggest gaps in my data science skillset. I decided to rectify this last year and did some online courses and reading to try get up to speed on simple neural networks, and more complex architecture such as Convolutional Neural Networks and Recurrent Neural Networks. I am really excited by the opportunities these new methods can offer ecology, and already have a number of cool projects fizzing in my brain.\nArmed with a basic understanding of how these things work. I decided to have a go at my first real-world application. Any ecologist working in the wonderous fynbos biome can attest to my frustration at trying to identify which of the more than 9000 possible species of plants they are currently holding in their hand. I am particularly rubbish at this task, but I am less rubbish at data science. Therefore I came about this project of trying to compensate for my poor botany skills by building a Neural Network that can automatically identify fynbos plant species from photos.\nStep 1: Getting the Data\nGetting photos of plants with confirmed ID’s can be a painstaking process. Luckily a vast collection of images of fynbos plants taken by amateur naturalists with confirmed ID’s has recently become available on iNaturalist. The best thing about this amazing resource (if you are a data scientist), is that it provides comprehensive access to all it’s data through a REST API. Even better is that there is a simple R package rinat through which the API can be called from R. For the beta version of my fynbos plant ID neural network I will just obtain photos of Proteas and Restios, two of the most common and iconic plant groups in the fynbos biome, and try to distinguish among these groups. In case you are totally unfamiliar with these plant groups\nHere is a Restio (family Restionaceae): Elegia filacea. photo: Ronald Flipphi CC BY-NC\nHere is a Protea (genus Protea): Protea eximia. photo: Gawie Malan CC BY-NC\nTo download images of family Restionaceae and genus Protea within a bounding box of the fynbos biome\n\n\n#load rinit library\nlibrary(rinat)\n\nsetwd(\"~/science/image_net\")\n\n#rough bounding box for the fynbos biome\nbounds <- c(-32.86,20,-34.3,24)\n\n#call iNaturalist API\nProteas <- get_inat_obs(taxon_name = \"Protea\",bounds=bounds)\nRestios <- get_inat_obs(taxon_name = \"Restionaceae\",bounds=bounds)\n\n\n#download the images for each group to a local file\n#restios\nfor (i  in length(Restios$image_url):1){\n  download.file(url = Restios$image_url[i],destfile = paste0(\"~/science/image_net/Data/restio/restio_\",Restios$id[i],\".jpg\"),method=\"libcurl\")\n}\n\n#proteas\nfor (i  in 1:length(Proteas$image_url)){\ndownload.file(url = Proteas$image_url[i],destfile = paste0(\"~/science/image_net/Data/protea/protea_\",i,\".jpg\"),method=\"libcurl\")\n}\n\nand just like that I have 100 images with confirmed identification for various Proteas and 100 images with confirmed identification for Restios. We could have more images, iNaturalist just limits us to 100 results per request.\nStep 2: Train the Neural Network\nI used a Convolutional Neural Network (CNN) to model the images. This type of Neural Network is commonly used for images analysis. This article on DataCamp give a great intro to CNN’s. I also used transfer learning to build my model on top of a pre-trained CNN fitted to a large dataset of images. Transfer learning uses predefined network structures with fixed weights to improve the fit of models on small datasets. Learn more about transfer learning at DataCamp. The logic is that these pretrained models, designed by very smart people and fitted to giant datasets have ‘learned’ the simple structure of images and can already identify features such as edges and corners. These features are captured in the basal layers of the network, which we inherit. To obtain a good fit to our data all we then do is add a few layers on top of the pretrained network that use the features it already knows to match our classes.\nThe dataset of 200 images of Proteas and Restios is not a particularly large dataset on which to train a CNN. To address this I used a technique called ‘image augmentation’. Essentially what this does is perform a series of transformations to our image dataset such as flipping, shifting, rescaling, zooming etc. to create new images which we can add to our dataset. I am not going to give you the full details here. You can read the tutorial on how to do this by François Chollet on the Keras Blog, you can also find the full code that I used for model fitting on my GitHub. Model fitting was done using Keras 2.0 with tensorflow backend in Python 2.7\nFitting the Proteas vs Restios image classifier with Keras 2.0\nAfter fine tuning these results a bit more the final model gives us about 80% accuracy. This is not exceptional - it is not going to replace a botanist just yet. But when you take a look at how diverse the images it is trying to identify are:\nSome Restios in the validation dataset\nSome Proteas in the validation dataset\nit is actually quite impressive. If we had a little standardization in the way we photographed the plants I think we could be getting 95% plus. An interesting side thought would be to train the classifier separately for images of flowers and images of vegetative plant material. “But how would we label images as flowers or vegetative?” you may ask. With a Neural Network of course.\nStep 3: Predict new images\nFinally, now that we have a trained model saved we can write a simple Python script that will load the model, and classify it as ‘Protea’ or ‘Restio’ given an image file.\n\n\n#!/usr/bin/python\n\nimport sys\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom keras.models import load_model\nfrom keras.models import Sequential\nimport cv2\nimport numpy as np \n\n#load saved model\nmodel = load_model('proteas_final.h5')\n\nphoto = str(sys.argv[1])\n\n#resize and classify image\ndef fynbos_id(model, photo):\n    img = cv2.imread(photo).astype(np.float32) / 255\n    img = cv2.resize(img, (150, 150))\n    img = np.expand_dims(img, axis=0)\n    classes = model.predict(img)\n    if classes <0.5:\n        sp_id = \"Protea\"\n    else:\n        sp_id = \"Restio\"      \n    return sp_id;\n\n#print result\nprint(fynbos_id(model,photo))\n\nLets try with a few new images not in either the training or validation set:\ntest_01.jpg\n\n\npython proteas_predict_single.py test_01.jpg\n\nreturns…. Restio\nYay!\ntest_02.jpg\n\n\npython proteas_predict_single.py test_02.jpg\n\nreturns…. Protea\nYay!\ntest_03.jpg\n\n\npython proteas_predict_single.py test_03.jpg\n\nreturns…. Protea\nHmmm\nso it’s not perfect, but not bad for a start. We need to add more images to the training and validation set to improve this model and maybe be more careful with the types of images we use to train with. But overall I think I have succeeded in this proof of concept. In the final stage of this workflow I will serve this model through a REST API using Python flask hosted on AWS. Look out for this in my next blog post.\n\n\n",
    "preview": "posts/fynbos_id/king-protea.jpg",
    "last_modified": "2022-10-07T11:52:35+02:00",
    "input_file": {}
  },
  {
    "path": "posts/emma/",
    "title": "EMMA wins UN data for climate action challenge",
    "description": "Predicting postfire reovery in the fynbos biome and monitoring anomalies",
    "author": [
      {
        "name": "Glenn Moncrieff",
        "url": {}
      }
    ],
    "date": "2017-11-15",
    "categories": [
      "Geospatial"
    ],
    "contents": "\nThe natural vegetation that grows around Cape Town, South Africa – the Cape Floristic Region (CFR) - is one of the richest repositories of plant life in the world. Here, about 20 percent of Africa’s flora grows in a landscape that accounts for less than 0.5 percent of the continent’s area. The diversity of plant life is among the highest on the planet. About 69 percent of the region’s estimated 9,000 plant species live nowhere else in the world.\nimage: Adam Wilson\nBut this flora is under severe pressure from a number of threats including climate change. Cape Town is currently experiencing its worst drought in living memory, with the city forecasted to run out of water by late autumn - before the winter rains arrive. This drought has been exacerbated by climate change, and ecologists suspect that vegetation is about to buckle under the pressure.\nBut suspicions are often all they have. The mountainous landscape and vast area make it practically impossible to manually assess the vegetation health of the entire area. This same problem is faced in the world’s forests. However, some amazing monitoring tools that use the rich satellite data made available by organizations like NASA and ESA are being used to assess the health of the world’s forests in real-time. Global Forest Watch is one such example.\nThe necessity to harness big data to contribute towards solving societies’ most pressing issues is obvious. To this end the Executive Office of the United Nations Secretary-General launched Global Pulse in 2009, in an attempt to bring real-time monitoring and prediction to development and aid programs.\nGlobal Pulse runs innovation programmes partnering with organizations that own or have access to big data, data analytics technologies, and data science expertise. In 2017 they announced the Data for Climate Action Challenge – an innovation initiative aimed at bringing the brightest minds, best data, and cutting edge tools to address the most pressing environmental challenge of our age – climate change. And when I reflect on the potential destructive impacts of climate change, one of my first considerations is its effect on the natural world.\nUniversity of Buffalo ecologist Adam Wilson, ecologist Jasper Slingsby, at the South African Environmental Observation Network (SAEON) and myself recognized how a monitoring tool would be of use in the CFR, and how it could be complemented by the inclusion of data collected by the public. Building on earlier research by Wilson that describes how healthy vegetation ought to behave when using satellites to monitor vegetation activity, we submitted our idea to the Data for Climate Action Challenge.\nOur submission included a lengthy description of how we envisage such a system to work, and a web app that allows users to view how the activity of natural vegetation as detected by satellites, compares to our prediction for healthy ecosystems. The mobile app allows citizen scientists to collect field data to aid in the assessment of our system’s accuracy.\nA Screenshot of the EMMA web application\nThe Data for Climate Action challenge selected 97 semi-finalist teams to develop projects using donated datasets and tools from 11 companies. Our system used high resolution satellite imagery from Planet Labs, social media data from Crimson Hexagon, cloud computing resources from Microsoft Azure and weather sensor data from Schneider Electric.\nOur submission, the Ecosystem Monitoring and Management Application or EMMA, was selected as one of three thematic winners in the Data for Climate Action challenge. Adam Wilson traveled to Bonn, Germany to receive the award on November 12 at a companion event to the Sustainable Innovation Forum, a side event to COP23, the annual U.N. Climate Change Conference taking place from November 6-17.\nI am very proud of my involvement in this project, and hope that this award and recognition of our tool will enable us to further develop our ideas into a fully fledged real-time monitoring system used by land management authorities and citizen scientists throughout the region, improving knowledge of climate change impacts on the Cape Floristic Region and aiding in species conservation.\n\n\n",
    "preview": "posts/emma/emma_cover.png",
    "last_modified": "2022-10-07T11:52:35+02:00",
    "input_file": {}
  },
  {
    "path": "posts/dc-ethiopia/",
    "title": "Data carpentry Ethiopia",
    "description": "Teaching R and data literacy in Addis Ababa",
    "author": [
      {
        "name": "Glenn Moncrieff",
        "url": {}
      }
    ],
    "date": "2017-10-02",
    "categories": [
      "Teaching"
    ],
    "contents": "\nThe Ethiopian Education and Research Network (EthERNet) from the Ministry of Education in collaboration with the German International Cooperation (GIZ) Sustainable Training and Education Programme (STEP), the Education Strategy Center (ESC) and Talarify organised its first ever Data Carpentry workshop for young academics and researchers in Ethiopia. The workshop was conducted over two and a half days from 14-16 August 2017 at Addis Ababa Institute of Technology (AAiT).\nThe main aim was to increase data literacy for researchers and establish a community of good research data practice in Ethiopia in order to increase the presence of Ethiopian researchers in the global research community. Note: UNESCO Statistics Institute reveals that in 2016 1.1% of the global research community are researchers coming from Sub-Saharan Africa. On average 30.4% from all Sub-Saharan researchers are females, whereas Ethiopia counts 13.3 % female researchers out of all Ethiopian researchers.\n\nOver 25 participants from all over Ethiopia joined the workshop. 98% of participants were women representing different research disciplines including animal nutrition, soil sciences, economics, sport sciences and information technology to name a few. The event was lead by Data Carpentry instructors from South Africa with helpers from Ethiopia and mainly covered lessons included in the Data Carpentry Ecology workshop - better use of Spreadsheets, data cleaning in OpenRefine, and data analysis and visualisation in R.\nAlong with my co-instuctor Lactatia Motsuku from the South African National Cancer Registry and lead instructor Anelda van der Walt from Talarify we delivered a great workshop that was warmly recevied from participants. Some of our Ethiopian workshops helpers have gone on to quaility as Software Carpentry instructors subsequent to the workshop, further growing the community in Africa.\nThis was my first experience instructing a data carpentry workshop and I have been inspired to become more involved in the movement after seeing how empowering researchers with the tools to do data driven reasearch can democratise science.\n\n\n",
    "preview": "posts/dc-ethiopia/dc_cover.png",
    "last_modified": "2022-10-07T11:52:35+02:00",
    "input_file": {}
  }
]
